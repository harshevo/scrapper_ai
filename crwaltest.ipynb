{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: crawl4ai in ./env/lib/python3.11/site-packages (0.4.1)\n",
      "Requirement already satisfied: aiosqlite~=0.20 in ./env/lib/python3.11/site-packages (from crawl4ai) (0.20.0)\n",
      "Requirement already satisfied: lxml~=5.3 in ./env/lib/python3.11/site-packages (from crawl4ai) (5.3.0)\n",
      "Requirement already satisfied: litellm>=1.53.1 in ./env/lib/python3.11/site-packages (from crawl4ai) (1.54.1)\n",
      "Requirement already satisfied: numpy<3,>=1.26.0 in ./env/lib/python3.11/site-packages (from crawl4ai) (2.2.0)\n",
      "Requirement already satisfied: pillow~=10.4 in ./env/lib/python3.11/site-packages (from crawl4ai) (10.4.0)\n",
      "Requirement already satisfied: playwright>=1.49.0 in ./env/lib/python3.11/site-packages (from crawl4ai) (1.49.0)\n",
      "Requirement already satisfied: python-dotenv~=1.0 in ./env/lib/python3.11/site-packages (from crawl4ai) (1.0.1)\n",
      "Requirement already satisfied: requests~=2.26 in ./env/lib/python3.11/site-packages (from crawl4ai) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4~=4.12 in ./env/lib/python3.11/site-packages (from crawl4ai) (4.12.3)\n",
      "Requirement already satisfied: tf-playwright-stealth>=1.1.0 in ./env/lib/python3.11/site-packages (from crawl4ai) (1.1.0)\n",
      "Requirement already satisfied: xxhash~=3.4 in ./env/lib/python3.11/site-packages (from crawl4ai) (3.5.0)\n",
      "Requirement already satisfied: rank-bm25~=0.2 in ./env/lib/python3.11/site-packages (from crawl4ai) (0.2.2)\n",
      "Requirement already satisfied: aiofiles>=24.1.0 in ./env/lib/python3.11/site-packages (from crawl4ai) (24.1.0)\n",
      "Requirement already satisfied: colorama~=0.4 in ./env/lib/python3.11/site-packages (from crawl4ai) (0.4.6)\n",
      "Requirement already satisfied: snowballstemmer~=2.2 in ./env/lib/python3.11/site-packages (from crawl4ai) (2.2.0)\n",
      "Requirement already satisfied: pydantic>=2.10 in ./env/lib/python3.11/site-packages (from crawl4ai) (2.10.3)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in ./env/lib/python3.11/site-packages (from aiosqlite~=0.20->crawl4ai) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./env/lib/python3.11/site-packages (from beautifulsoup4~=4.12->crawl4ai) (2.6)\n",
      "Requirement already satisfied: aiohttp in ./env/lib/python3.11/site-packages (from litellm>=1.53.1->crawl4ai) (3.11.10)\n",
      "Requirement already satisfied: click in ./env/lib/python3.11/site-packages (from litellm>=1.53.1->crawl4ai) (8.1.7)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.23.0 in ./env/lib/python3.11/site-packages (from litellm>=1.53.1->crawl4ai) (0.27.2)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in ./env/lib/python3.11/site-packages (from litellm>=1.53.1->crawl4ai) (8.5.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in ./env/lib/python3.11/site-packages (from litellm>=1.53.1->crawl4ai) (3.1.4)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in ./env/lib/python3.11/site-packages (from litellm>=1.53.1->crawl4ai) (4.23.0)\n",
      "Requirement already satisfied: openai>=1.55.3 in ./env/lib/python3.11/site-packages (from litellm>=1.53.1->crawl4ai) (1.57.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in ./env/lib/python3.11/site-packages (from litellm>=1.53.1->crawl4ai) (0.8.0)\n",
      "Requirement already satisfied: tokenizers in ./env/lib/python3.11/site-packages (from litellm>=1.53.1->crawl4ai) (0.21.0)\n",
      "Requirement already satisfied: greenlet==3.1.1 in ./env/lib/python3.11/site-packages (from playwright>=1.49.0->crawl4ai) (3.1.1)\n",
      "Requirement already satisfied: pyee==12.0.0 in ./env/lib/python3.11/site-packages (from playwright>=1.49.0->crawl4ai) (12.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./env/lib/python3.11/site-packages (from pydantic>=2.10->crawl4ai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in ./env/lib/python3.11/site-packages (from pydantic>=2.10->crawl4ai) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.11/site-packages (from requests~=2.26->crawl4ai) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.11/site-packages (from requests~=2.26->crawl4ai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.11/site-packages (from requests~=2.26->crawl4ai) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.11/site-packages (from requests~=2.26->crawl4ai) (2024.8.30)\n",
      "Requirement already satisfied: fake-http-header<0.4.0,>=0.3.5 in ./env/lib/python3.11/site-packages (from tf-playwright-stealth>=1.1.0->crawl4ai) (0.3.5)\n",
      "Requirement already satisfied: pytest-mockito<0.0.5,>=0.0.4 in ./env/lib/python3.11/site-packages (from tf-playwright-stealth>=1.1.0->crawl4ai) (0.0.4)\n",
      "Requirement already satisfied: anyio in ./env/lib/python3.11/site-packages (from httpx<0.28.0,>=0.23.0->litellm>=1.53.1->crawl4ai) (4.7.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./env/lib/python3.11/site-packages (from httpx<0.28.0,>=0.23.0->litellm>=1.53.1->crawl4ai) (1.0.7)\n",
      "Requirement already satisfied: sniffio in ./env/lib/python3.11/site-packages (from httpx<0.28.0,>=0.23.0->litellm>=1.53.1->crawl4ai) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./env/lib/python3.11/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.23.0->litellm>=1.53.1->crawl4ai) (0.14.0)\n",
      "Requirement already satisfied: zipp>=3.20 in ./env/lib/python3.11/site-packages (from importlib-metadata>=6.8.0->litellm>=1.53.1->crawl4ai) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.11/site-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.53.1->crawl4ai) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./env/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./env/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./env/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./env/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (0.22.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./env/lib/python3.11/site-packages (from openai>=1.55.3->litellm>=1.53.1->crawl4ai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./env/lib/python3.11/site-packages (from openai>=1.55.3->litellm>=1.53.1->crawl4ai) (0.8.2)\n",
      "Requirement already satisfied: tqdm>4 in ./env/lib/python3.11/site-packages (from openai>=1.55.3->litellm>=1.53.1->crawl4ai) (4.67.1)\n",
      "Requirement already satisfied: pytest>=3 in ./env/lib/python3.11/site-packages (from pytest-mockito<0.0.5,>=0.0.4->tf-playwright-stealth>=1.1.0->crawl4ai) (8.3.4)\n",
      "Requirement already satisfied: mockito>=1.0.6 in ./env/lib/python3.11/site-packages (from pytest-mockito<0.0.5,>=0.0.4->tf-playwright-stealth>=1.1.0->crawl4ai) (1.5.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./env/lib/python3.11/site-packages (from tiktoken>=0.7.0->litellm>=1.53.1->crawl4ai) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./env/lib/python3.11/site-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./env/lib/python3.11/site-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./env/lib/python3.11/site-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./env/lib/python3.11/site-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./env/lib/python3.11/site-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./env/lib/python3.11/site-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (1.18.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./env/lib/python3.11/site-packages (from tokenizers->litellm>=1.53.1->crawl4ai) (0.26.5)\n",
      "Requirement already satisfied: filelock in ./env/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./env/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./env/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./env/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (6.0.2)\n",
      "Requirement already satisfied: iniconfig in ./env/lib/python3.11/site-packages (from pytest>=3->pytest-mockito<0.0.5,>=0.0.4->tf-playwright-stealth>=1.1.0->crawl4ai) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in ./env/lib/python3.11/site-packages (from pytest>=3->pytest-mockito<0.0.5,>=0.0.4->tf-playwright-stealth>=1.1.0->crawl4ai) (1.5.0)\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "Requirement already satisfied: nest_asyncio in ./env/lib/python3.11/site-packages (1.6.0)\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "Playwright Host validation warning: \n",
      "╔══════════════════════════════════════════════════════╗\n",
      "║ Host system is missing dependencies to run browsers. ║\n",
      "║ Please install them with the following command:      ║\n",
      "║                                                      ║\n",
      "║     sudo playwright install-deps                     ║\n",
      "║                                                      ║\n",
      "║ Alternatively, use apt:                              ║\n",
      "║     sudo apt-get install libevent-2.1-7t64           ║\n",
      "║                                                      ║\n",
      "║ <3 Playwright Team                                   ║\n",
      "╚══════════════════════════════════════════════════════╝\n",
      "    at validateDependenciesLinux (/home/prakhar/Documents/strollr/crawlai/env/lib/python3.11/site-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
      "    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n",
      "    at async Registry._validateHostRequirements (/home/prakhar/Documents/strollr/crawlai/env/lib/python3.11/site-packages/playwright/driver/package/lib/server/registry/index.js:753:43)\n",
      "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/home/prakhar/Documents/strollr/crawlai/env/lib/python3.11/site-packages/playwright/driver/package/lib/server/registry/index.js:851:7)\n",
      "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/home/prakhar/Documents/strollr/crawlai/env/lib/python3.11/site-packages/playwright/driver/package/lib/server/registry/index.js:840:43)\n",
      "    at async t.<anonymous> (/home/prakhar/Documents/strollr/crawlai/env/lib/python3.11/site-packages/playwright/driver/package/lib/cli/program.js:137:7)\n",
      "Requirement already satisfied: tavily-python in ./env/lib/python3.11/site-packages (0.5.0)\n",
      "Requirement already satisfied: requests in ./env/lib/python3.11/site-packages (from tavily-python) (2.32.3)\n",
      "Requirement already satisfied: tiktoken>=0.5.1 in ./env/lib/python3.11/site-packages (from tavily-python) (0.8.0)\n",
      "Requirement already satisfied: httpx in ./env/lib/python3.11/site-packages (from tavily-python) (0.27.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./env/lib/python3.11/site-packages (from tiktoken>=0.5.1->tavily-python) (2024.11.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.11/site-packages (from requests->tavily-python) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.11/site-packages (from requests->tavily-python) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.11/site-packages (from requests->tavily-python) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.11/site-packages (from requests->tavily-python) (2024.8.30)\n",
      "Requirement already satisfied: anyio in ./env/lib/python3.11/site-packages (from httpx->tavily-python) (4.7.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./env/lib/python3.11/site-packages (from httpx->tavily-python) (1.0.7)\n",
      "Requirement already satisfied: sniffio in ./env/lib/python3.11/site-packages (from httpx->tavily-python) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./env/lib/python3.11/site-packages (from httpcore==1.*->httpx->tavily-python) (0.14.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in ./env/lib/python3.11/site-packages (from anyio->httpx->tavily-python) (4.12.2)\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install crawl4ai\n",
    "!pip install nest_asyncio\n",
    "!playwright install\n",
    "!pip install tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./env/lib/python3.11/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./env/lib/python3.11/site-packages (from ipywidgets) (8.30.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./env/lib/python3.11/site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.12 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: decorator in ./env/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./env/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in ./env/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in ./env/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./env/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./env/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack_data in ./env/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in ./env/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./env/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./env/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./env/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./env/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./env/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in ./env/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.4/214.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 widgetsnbextension-4.0.13\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'kids activities in sydney', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Things to Do with Kids in Sydney | Family-Friendly Fun - Time Out', 'url': 'https://www.timeout.com/sydney/kids', 'content': \"Your guide to Sydney's best things to do with kids, family-friendly restaurants, kids' shows, kids' shops and more. ... Activities for kids in Sydney. Kids. The best playgrounds in Sydney.\", 'score': 0.9992592, 'raw_content': None}, {'title': '26 Of The Best Things To Do With Kids In Sydney - Urban List', 'url': 'https://www.theurbanlist.com/sydney/a-list/things-to-do-with-kids-sydney', 'content': \"Best Adventure Activities With Kids In Sydney BridgeClimb Sydney 3 Cumberland Street, The Rocks. Image credit: BridgeClimb Sydney | Instagram . Kids over the age of eight and in need of something a little more exciting than Lego can join the adults on the famous BridgeClimb Sydney. Ascend as a family up the Harbour Bridge (aka the world's\", 'score': 0.9982658, 'raw_content': None}, {'title': 'Things to do in Sydney with kids - Sydney for kids | Sydney.com', 'url': 'https://www.sydney.com/things-to-do/family-holidays/top-attractions-for-kids', 'content': 'Find things to do with kids in Sydney. Explore family activities, places to visit & attractions for kids including zoos, aquariums, beaches, parks, museums & more. ... t ake a ferry from Circular Quay across sparkling Sydney Harbour so kids can also take in the Sydney Opera House from a new angle. The zoo has hundreds of animals to see, like', 'score': 0.9979412, 'raw_content': None}, {'title': \"Things to do in Sydney with kids | City of Sydney - What's On\", 'url': 'https://whatson.cityofsydney.nsw.gov.au/articles/things-to-do-in-sydney-with-kids', 'content': \"Things to do in Sydney with kids | City of Sydney - What’s On Things to do in Sydney with kids Get dirty at Sydney Park If your kids love to skate or want to learn, the amazing skate park in Sydney Park in Alexandria is a brilliant choice. 8. Get dirty at Sydney Park Sydney Park offers 40 hectares of greenery a stone's throw from the city centre. Plus, if you have a little\\xa0one learning how to ride a bike, Sydney Park's cycling centre has a terrific course for kids aged 3 to 8. Sydney Park's super fun playground Take your kids to see public art\\xa0and historical sites in Sydney, and uncover our city's stories.\", 'score': 0.99783427, 'raw_content': None}, {'title': \"Children | Events in Sydney | City of Sydney - What's On\", 'url': 'https://whatson.cityofsydney.nsw.gov.au/tags/children', 'content': \"Sydney Sydney Sydney Dance Company: children and families classes Sydney Sydney Dance Company: children and families classes Art club for kids @ Ultimo Community Centre, 6-9yrs Art club for kids @ Ultimo Community Centre, 6-9yrs Sydney Join us for our annual summer open day with free entry to all City of Sydney aquatic and recreation centres. Join us at Santa's Workshop Kingsford for the first ever Black Santa photos in Sydney! Family Christmas wreath making workshop Family Christmas wreath making workshop Join our family workshop to create with your child a handmade, Christmas wreath! Join us for Santa’s Studio Christmas craft workshop Two days of free craft workshops, sing-a-longs, lawn games, dance parties and a whole lot of family fun! Sydney\", 'score': 0.99760324, 'raw_content': None}], 'response_time': 1.77}\n"
     ]
    }
   ],
   "source": [
    "from tavily import TavilyClient\n",
    "import os\n",
    "# import google.generativeai as genai\n",
    "\n",
    "# model = genai.GenerativeModel(\"gemini-1.5-pro-latest\")\n",
    "# prompt = \"\"\"List a few popular cookie recipes in JSON format.\n",
    "\n",
    "# Use this JSON schema:\n",
    "\n",
    "# Recipe = {'recipe_name': str, 'ingredients': list[str]}\n",
    "# Return: list[Recipe]\"\"\"\n",
    "# result = model.generate_content(prompt)\n",
    "# print(result)\n",
    "\n",
    "\n",
    "# Step 1. Instantiating your TavilyClient\n",
    "tavily_client = TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
    "\n",
    "# Step 2. Executing a simple search query\n",
    "response = tavily_client.search(\"kids activities in sydney\")\n",
    "\n",
    "# Step 3. That's it! You've done a Tavily Search!\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing_extensions as typing\n",
    "import google.generativeai as genai\n",
    "from config import Settings\n",
    "\n",
    "settings = Settings()\n",
    "\n",
    "\n",
    "async def get_final_structured_data_from_content():\n",
    "    genai.configure(api_key=settings.OPENAI_API_KEY)\n",
    "\n",
    "    # class OutputFormat(typing.TypedDict):\n",
    "    #     business_name: str\n",
    "    #     address: str\n",
    "    #     phone_number: str\n",
    "    #     email: str\n",
    "    #     website_link: str\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    Say Hi\n",
    "    \"\"\"\n",
    "\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-pro-latest\")\n",
    "    result = model.generate_content(\n",
    "        prompt,\n",
    "        generation_config=genai.GenerationConfig(\n",
    "            response_mime_type=\"application/json\",\n",
    "            # response_schema=list[OutputFormat],\n",
    "            temperature=0.7,\n",
    "            candidate_count=1,\n",
    "            max_output_tokens=1024,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    print(result.text)\n",
    "    return result.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query [{\"query\": \"Arts & Crafts classes Play Centre 2839 NSW\", \"query_number\": 1}, {\"query\": \"Book Babies sessions Library Griffith NSW 2839\", \"query_number\": 2}, {\"query\": \"Kids Dance Classes Community Centre near 2839\", \"query_number\": 3}, {\"query\": \"Children's Events Playground Griffith NSW 2839\", \"query_number\": 4}, {\"query\": \"Gymnastics classes for toddlers near 2839 NSW\", \"query_number\": 5}, {\"query\": \"Learn Languages programs Library 2839 postcode\", \"query_number\": 6}, {\"query\": \"Mums n Bubs Fitness Classes Griffith 2839 booking\", \"query_number\": 7}, {\"query\": \"Music Classes for preschoolers near postcode 2839\", \"query_number\": 8}, {\"query\": \"Playgroup sessions Community Centre 2839 NSW\", \"query_number\": 9}, {\"query\": \"Playroom activities Toy Library near Griffith 2839\", \"query_number\": 10}, {\"query\": \"Rhyme Time schedule Library 2839 NSW\", \"query_number\": 11}, {\"query\": \"Sensory Classes for babies near 2839\", \"query_number\": 12}, {\"query\": \"Singing Lessons for kids Griffith 2839 NSW\", \"query_number\": 13}, {\"query\": \"Swimming Lessons Aquatic Centre 2839 postcode\", \"query_number\": 14}, {\"query\": \"Toddler Time activities Play Centre near 2839\", \"query_number\": 15}, {\"query\": \"Kids activities Museum Griffith NSW 2839\", \"query_number\": 16}, {\"query\": \"Preschool programs Community Centre 2839\", \"query_number\": 17}, {\"query\": \"Children's Events and activities near 2839\", \"query_number\": 18}, {\"query\": \"Baby and toddler classes Griffith 2839 region\", \"query_number\": 19}, {\"query\": \"Kids entertainment and programs near postcode 2839\", \"query_number\": 20}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.11/re/__init__.py:258: RuntimeWarning: coroutine 'main' was never awaited\n",
      "  return pattern.translate(_special_chars_map)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'GenerateContentResponse' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 158\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal urls\u001b[39m\u001b[38;5;124m\"\u001b[39m, urls)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/strollr/crawlai/env/lib/python3.11/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/Documents/strollr/crawlai/env/lib/python3.11/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/asyncio/futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/usr/lib/python3.11/asyncio/tasks.py:277\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[0;32mIn[3], line 151\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m urls \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m query:\n\u001b[0;32m--> 151\u001b[0m     urls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m get_relevant_urls(\u001b[43mq\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    152\u001b[0m     urls\u001b[38;5;241m.\u001b[39mextend(urls)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal urls\u001b[39m\u001b[38;5;124m\"\u001b[39m, urls)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'GenerateContentResponse' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from tavily import TavilyClient\n",
    "from crawl4ai import AsyncWebCrawler, CacheMode\n",
    "import google.generativeai as genai\n",
    "\n",
    "import typing_extensions as typing\n",
    "\n",
    "\n",
    "async def query_generation(location: str, postcode: str):\n",
    "    genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "    keywords = [\n",
    "        \"Arts & Crafts\",\n",
    "        \"Book Babies\",\n",
    "        \"Dance Classes\",\n",
    "        \"Events\",\n",
    "        \"Gymnastics\",\n",
    "        \"Learn Languages\",\n",
    "        \"Mums n Bubs Fitness Classes\",\n",
    "        \"Music Classes\",\n",
    "        \"Playgroup\",\n",
    "        \"Playroom\",\n",
    "        \"Rhyme Time\",\n",
    "        \"Sensory Classes\",\n",
    "        \"Singing Lessons\",\n",
    "        \"Swimming Lessons\",\n",
    "        \"Toddler Time\",\n",
    "        \"Other\",\n",
    "    ]\n",
    "    venue = [\n",
    "        \"Aquatic Centre\",\n",
    "        \"Library\",\n",
    "        \"Museum\",\n",
    "        \"Play Centre\",\n",
    "        \"Playground\",\n",
    "        \"Toy Library\",\n",
    "        \"Other\",\n",
    "        \"Community Centre\",\n",
    "    ]\n",
    "\n",
    "    class OutputFormat(typing.TypedDict):\n",
    "        query_number: int\n",
    "        query: str\n",
    "\n",
    "    prompt = \"\"\"Generate 20 different search queries to find websites, listings, and information about kids activities in {location} with postcode {postcode}. \n",
    "    Each query should combine activities from this list: {keywords}\n",
    "    with venues from this list: {venue}\n",
    "    \n",
    "    Follow these rules for effective search queries:\n",
    "    - Include specific Sydney locations or suburbs when relevant\n",
    "    - Add search-relevant terms like \"classes\", \"schedule\", \"booking\", \"programs\"\n",
    "    - Include age groups (toddler, preschool, kids) where appropriate\n",
    "    - Make queries specific enough to find actual programs, not general information\n",
    "    - Format: \"primary activity/topic\" + \"location/venue\" + \"relevant qualifiers\"\n",
    "    \n",
    "    Examples:\n",
    "    - \"toddler swimming lessons aquatic centre eastern suburbs sydney schedule\"\n",
    "    - \"kids art classes community centres inner west sydney bookings\"\n",
    "    \"\"\".format(keywords=keywords, venue=venue, location=location, postcode=postcode)\n",
    "\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-pro-latest\")\n",
    "    result = model.generate_content(\n",
    "        prompt,\n",
    "        generation_config=genai.GenerationConfig(\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=list[OutputFormat],\n",
    "            temperature=0.7,\n",
    "            candidate_count=1,\n",
    "            max_output_tokens=1024,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    print(\"query\", result.text)  # Print the generated queries\n",
    "    return result\n",
    "\n",
    "\n",
    "async def get_relevant_urls(query: str):\n",
    "    # Step 1. Instantiating your TavilyClient\n",
    "    tavily_client = TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
    "\n",
    "    # Step 2. Executing a simple search query\n",
    "    response = tavily_client.search(query)\n",
    "\n",
    "    # Step 3. That's it! You've done a Tavily Search!\n",
    "    print(\"response tavily\", response)\n",
    "    return response\n",
    "\n",
    "\n",
    "async def crawl_dynamic_content():\n",
    "    async with AsyncWebCrawler(verbose=True) as crawler:\n",
    "        result = await crawler.arun(\n",
    "            url=\"https://www.timeout.com/sydney/kids\",\n",
    "            # Content filtering\n",
    "            word_count_threshold=10,\n",
    "            excluded_tags=[\"form\", \"header\", \"nav\", \"footer\", \"aside\"],\n",
    "            exclude_external_links=False,\n",
    "            # Content processing\n",
    "            process_iframes=False,\n",
    "            remove_overlay_elements=True,\n",
    "            # Cache control\n",
    "            cache_mode=CacheMode.ENABLED,  # Use cache if available\n",
    "            magic=True,  # Enable all anti-detection features\n",
    "            # simulate_user=True,  # Simulate human behavior\n",
    "            # override_navigator=True,  # Override navigator properties\n",
    "        )\n",
    "\n",
    "        content = None\n",
    "\n",
    "        if result.success:\n",
    "            content = {\n",
    "                # \"title\": result.title,\n",
    "                \"url\": result.url,\n",
    "                \"content\": result.markdown,\n",
    "                \"images\": result.media[\"images\"],\n",
    "                \"links\": result.links[\"internal\"],\n",
    "                \"external_links\": result.links[\"external\"],\n",
    "            }\n",
    "\n",
    "            print(content)\n",
    "\n",
    "            return content\n",
    "\n",
    "        # if result.success:\n",
    "        #     # Print clean content\n",
    "        #     print(\"Content:\", content)  # First 500 chars\n",
    "\n",
    "        #     # Process images\n",
    "        #     for image in result.media[\"images\"]:\n",
    "        #         print(f\"Found image: {image['src']}\")\n",
    "        #         content[\"images\"].append(image[\"src\"])\n",
    "\n",
    "        #     # Process links\n",
    "        #     for link in result.links[\"internal\"]:\n",
    "        #         print(f\"Internal link: {link['href']}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Crawl failed: {result.error_message}\")\n",
    "            return {\"error\": result.error_message}\n",
    "\n",
    "\n",
    "async def main():\n",
    "    # return await crawl_dynamic_content()\n",
    "    postcode = input(\"Enter postcode: \")\n",
    "    location = input(\"Enter location: \")\n",
    "    query = await query_generation(location=location, postcode=postcode)\n",
    "\n",
    "    urls = []\n",
    "\n",
    "    for q in query:\n",
    "        urls = await get_relevant_urls(q[\"query\"])\n",
    "        urls.extend(urls)\n",
    "\n",
    "    print(\"final urls\", urls)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT].... → Crawl4AI 0.4.1\n",
      "[FETCH]... ↓ https://www.timeout.com/sydney/kids... | Status: True | Time: 0.01s\n",
      "[SCRAPE].. ◆ Processed https://www.timeout.com/sydney/kids... | Time: 11ms\n",
      "[COMPLETE] ● https://www.timeout.com/sydney/kids... | Status: True | Total: 0.02s\n",
      "Content: [](/sydney/kids/school-holidays-in-sydney)\n",
      "\n",
      "##### Kids\n",
      "\n",
      "### [The best things to do with kids in Sydney](/sydney/kids/school-holidays-in-sydney)\n",
      "\n",
      "Our ultimate list of the best kid-friendly activities and experiences Sydney, including indoor activities for wet days.\n",
      "\n",
      "[](/sydney/kids/the-best-playgrounds-in-sydney)\n",
      "\n",
      "##### Kids\n",
      "\n",
      "### [The best playgrounds in Sydney](/sydney/kids/the-best-playgrounds-in-sydney)\n",
      "\n",
      "Got a wriggly little one? Let them run off some of that energy at these parks and playgrounds.\n",
      "\n",
      "[](/sydney/kids/the-best-kid-friendly-pubs-in-sydney)\n",
      "\n",
      "##### Bars\n",
      "\n",
      "### [The best kid-friendly pubs in Sydney](/sydney/kids/the-best-kid-friendly-pubs-in-sydney)\n",
      "\n",
      "We've rounded up the Sydney pubs that have the best facilities for kids and their families.\n",
      "\n",
      "Advertising\n",
      "\n",
      "## What's on for kids in Sydney\n",
      "\n",
      "[This week](/sydney/kids/kids-activities-sydney)\n",
      "\n",
      "[Rainy days](/sydney/kids/rainy-day-activities-for-kids)\n",
      "\n",
      "[School holidays](/sydney/kids/school-holidays-in-sydney)\n",
      "\n",
      "[Everyday fun](/sydney/kids/the-best-activities-for-kids-in-sydney)\n",
      "\n",
      "By entering your email address you agree to our [Terms of Use](https://www.timeout.com/terms-of-use) and [Privacy Policy](https://www.timeout.com/privacy-notice) and consent to receive emails from Time Out about news, events, offers and partner promotions.\n",
      "\n",
      "🙌 Awesome, you're subscribed!\n",
      "\n",
      "Thanks for subscribing! Look out for your first newsletter in your inbox soon!\n",
      "\n",
      "## Activities for kids in Sydney\n",
      "\n",
      "[](/sydney/kids/the-best-playgrounds-in-sydney)\n",
      "\n",
      "Kids\n",
      "\n",
      "### [The best playgrounds in Sydney](/sydney/kids/the-best-playgrounds-in-sydney)\n",
      "\n",
      "Got a wriggly little one? Let them run off some of that energy on these jungle gyms dotted in green spaces around the city.\n",
      "\n",
      "[](/sydney/attractions/where-to-see-animals-in-sydney)\n",
      "\n",
      "Attractions\n",
      "\n",
      "### [Where to see animals in Sydney](/sydney/attractions/where-to-see-animals-in-sydney)\n",
      "\n",
      "There’s many a stable, sanctuary, zoo or harbour beach that houses wildlife in Sydney.\n",
      "\n",
      "[](/sydney/kids/the-best-toy-shops-in-sydney)\n",
      "\n",
      "Kids\n",
      "\n",
      "### [The best toy shops in Sydney](/sydney/kids/the-best-toy-shops-in-sydney)\n",
      "\n",
      "Scour these shelves for carefully-selected prizes. \n",
      "\n",
      "[](/sydney/things-to-do/50-things-to-do-indoors)\n",
      "\n",
      "Things to do\n",
      "\n",
      "### [101 fun things to do indoors in Sydney](/sydney/things-to-do/50-things-to-do-indoors)\n",
      "\n",
      "Take shelter with our guide to the best indoor activities in Sydney. \n",
      "\n",
      "[](/sydney/attractions/where-to-find-the-best-sydney-parks)\n",
      "\n",
      "Attractions\n",
      "\n",
      "### [Where to find the best Sydney parks](/sydney/attractions/where-to-find-the-best-sydney-parks)\n",
      "\n",
      "While away the afternoon on the grass of these sprawling open spaces.\n",
      "\n",
      "Advertising\n",
      "\n",
      "## Family adventures in Sydney\n",
      "\n",
      "[](/sydney/things-to-do/dog-friendly-beaches)\n",
      "\n",
      "##### Attractions\n",
      "\n",
      "### [The best dog-friendly beaches in Sydney](/sydney/things-to-do/dog-friendly-beaches)\n",
      "\n",
      "Meet your dog-mad neighbours and let the kids and pup get sandy.\n",
      "\n",
      "[](/sydney/things-to-do/where-to-go-fruit-picking-near-sydney)\n",
      "\n",
      "##### Things to do\n",
      "\n",
      "### [Where to go fruit picking near Sydney](/sydney/things-to-do/where-to-go-fruit-picking-near-sydney)\n",
      "\n",
      "The whole fam can pick their five-a-day in the sunshine.\n",
      "\n",
      "[](/sydney/things-to-do/the-best-picnic-spots-in-sydney)\n",
      "\n",
      "##### Things to do\n",
      "\n",
      "### [The best picnic spots in Sydney](/sydney/things-to-do/the-best-picnic-spots-in-sydney)\n",
      "\n",
      "Wrap up the whole family for lunch on the grass.\n",
      "\n",
      "[Back to Top](#top)\n",
      "\n",
      "[](https://www.timeout.com/sydney)\n",
      "\n",
      "[](https://www.facebook.com/TimeOutSydney)[](https://twitter.com/TimeOutSydney)[](https://www.instagram.com/timeoutsydney)[](https://uk.pinterest.com/timeoutsydney/)\n",
      "\n",
      "[About us](https://www.timeout.com/about)\n",
      "\n",
      "  * [Acknowledgement of Country](https://www.timeout.com/sydney/acknowledgement-of-country)\n",
      "  * [Press office](https://www.timeout.com/about/latest-news)\n",
      "  * [Investor relations](https://www.timeout.com/about/investors)\n",
      "  * [Work for Time Out](https://media.au.timeout.com/careers)\n",
      "  * [Editorial guidelines](https://www.timeout.com/about/editorial-guidelines)\n",
      "  * [Privacy notice](https://www.timeout.com/privacy-notice)\n",
      "  * [Do not sell my information](https://www.timeout.com/privacy-notice#CCPA)\n",
      "  * [Cookie policy](https://www.timeout.com/cookie-policy)\n",
      "  * [Accessibility statement](https://www.timeout.com/accessibility-statement)\n",
      "  * [Terms of use](https://www.timeout.com/terms-of-use)\n",
      "  * [Reviews policy](https://www.timeout.com/sydney/reviews-policy)\n",
      "  * [Competition terms](https://www.timeout.com/sydney/competition-terms)\n",
      "  * [About the site](https://www.timeout.com/sydney/about-the-site)\n",
      "  * [Modern slavery statement](https://assets.timeout.com/application/pdf/72f4731237cff6b1e09555d8e586b3c1/modern_slavery_statement_2024_2025.pdf)\n",
      "  * [Manage cookies]()\n",
      "\n",
      "\n",
      "\n",
      "[Contact us](https://www.timeout.com/sydney/contact-us)\n",
      "\n",
      "  * [Editorial](https://www.timeout.com/sydney/contact-us)\n",
      "  * [Advertising](https://media.au.timeout.com)\n",
      "  * [Report an error](https://www.timeout.com/sydney/report-an-error)\n",
      "  * [Time Out Market](https://www.timeoutmarket.com/)\n",
      "\n",
      "\n",
      "\n",
      "Time Out products\n",
      "\n",
      "  * [Magazine](https://www.timeout.com/sydney/magazine)\n",
      "  * [Newsletter](https://www.timeout.com/sydney/newsletter#signup)\n",
      "  * [Time Out Worldwide](https://www.timeout.com)\n",
      "\n",
      "\n",
      "\n",
      "[Site map](/sydney/sitemaps)\n",
      "\n",
      "© 2024 Time Out England Limited and affiliated companies owned by Time Out Group Plc. All rights reserved. Time Out is a registered trademark of Time Out Digital Limited.\n",
      "\n",
      "Internal link: https://www.timeout.com/sydney/kids/school-holidays-in-sydney\n",
      "Internal link: https://www.timeout.com/sydney/kids/the-best-playgrounds-in-sydney\n",
      "Internal link: https://www.timeout.com/sydney/kids/the-best-kid-friendly-pubs-in-sydney\n",
      "Internal link: https://www.timeout.com/sydney/kids/kids-activities-sydney\n",
      "Internal link: https://www.timeout.com/sydney/kids/rainy-day-activities-for-kids\n",
      "Internal link: https://www.timeout.com/sydney/kids/the-best-activities-for-kids-in-sydney\n",
      "Internal link: https://www.timeout.com/terms-of-use\n",
      "Internal link: https://www.timeout.com/privacy-notice\n",
      "Internal link: https://www.timeout.com/sydney/attractions/where-to-see-animals-in-sydney\n",
      "Internal link: https://www.timeout.com/sydney/kids/the-best-toy-shops-in-sydney\n",
      "Internal link: https://www.timeout.com/sydney/things-to-do/50-things-to-do-indoors\n",
      "Internal link: https://www.timeout.com/sydney/attractions/where-to-find-the-best-sydney-parks\n",
      "Internal link: https://www.timeout.com/sydney/things-to-do/dog-friendly-beaches\n",
      "Internal link: https://www.timeout.com/sydney/things-to-do/where-to-go-fruit-picking-near-sydney\n",
      "Internal link: https://www.timeout.com/sydney/things-to-do/the-best-picnic-spots-in-sydney\n",
      "Internal link: https://www.timeout.com/sydney/kids#top\n",
      "Internal link: https://www.timeout.com/sydney\n",
      "Internal link: https://www.timeout.com/about\n",
      "Internal link: https://www.timeout.com/sydney/acknowledgement-of-country\n",
      "Internal link: https://www.timeout.com/about/latest-news\n",
      "Internal link: https://www.timeout.com/about/investors\n",
      "Internal link: https://www.timeout.com/about/editorial-guidelines\n",
      "Internal link: https://www.timeout.com/privacy-notice#CCPA\n",
      "Internal link: https://www.timeout.com/cookie-policy\n",
      "Internal link: https://www.timeout.com/accessibility-statement\n",
      "Internal link: https://www.timeout.com/sydney/reviews-policy\n",
      "Internal link: https://www.timeout.com/sydney/competition-terms\n",
      "Internal link: https://www.timeout.com/sydney/about-the-site\n",
      "Internal link: https://www.timeout.com/sydney/contact-us\n",
      "Internal link: https://www.timeout.com/sydney/report-an-error\n",
      "Internal link: https://www.timeout.com/sydney/magazine\n",
      "Internal link: https://www.timeout.com/sydney/newsletter#signup\n",
      "Internal link: https://www.timeout.com\n",
      "Internal link: https://www.timeout.com/sydney/sitemaps\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from crawl4ai import AsyncWebCrawler, CacheMode\n",
    "\n",
    "\n",
    "async def main():\n",
    "    async with AsyncWebCrawler(verbose=True) as crawler:\n",
    "        result = await crawler.arun(\n",
    "            url=\"https://www.timeout.com/sydney/kids\",\n",
    "            # Content filtering\n",
    "            word_count_threshold=10,\n",
    "            excluded_tags=[\"form\", \"header\", \"nav\", \"footer\", \"aside\"],\n",
    "            exclude_external_links=False,\n",
    "            # Content processing\n",
    "            process_iframes=False,\n",
    "            remove_overlay_elements=True,\n",
    "            # Cache control\n",
    "            cache_mode=CacheMode.ENABLED,  # Use cache if available\n",
    "            magic=True,  # Enable all anti-detection features\n",
    "            # simulate_user=True,  # Simulate human behavior\n",
    "            # override_navigator=True,  # Override navigator properties\n",
    "        )\n",
    "\n",
    "        if result.success:\n",
    "            # Print clean content\n",
    "            print(\"Content:\", result.markdown)  # First 500 chars\n",
    "\n",
    "            # Process images\n",
    "            for image in result.media[\"images\"]:\n",
    "                print(f\"Found image: {image['src']}\")\n",
    "\n",
    "            # Process links\n",
    "            for link in result.links[\"internal\"]:\n",
    "                print(f\"Internal link: {link['href']}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Crawl failed: {result.error_message}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result Toggle navigation\n",
      "\n",
      "[![Web Scraper](/img/logo_white.svg)](/)\n",
      "\n",
      "  * [ Web Scraper ](/)\n",
      "  * [ Cloud Scraper ](/cloud-scraper)\n",
      "  * [ Pricing ](/pricing)\n",
      "  * Learn\n",
      "\n",
      "    * [Documentation](/documentation)\n",
      "    * [Video Tutorials](/tutorials)\n",
      "    * [How to](/how-to-videos)\n",
      "    * [Test Sites](/test-sites)\n",
      "    * [Forum](https://forum.webscraper.io/)\n",
      "  * [Install](https://chromewebstore.google.com/detail/web-scraper/jnhgnonknehpejjnehehllkliplmbmhn?hl=en)\n",
      "  * [Cloud Login](https://cloud.webscraper.io/)\n",
      "\n",
      "\n",
      "\n",
      "# \n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "# from crawl4ai import AsyncWebCrawler\n",
    "\n",
    "\n",
    "# async def simple_crawl():\n",
    "#     async with AsyncWebCrawler() as crawler:\n",
    "#         result = await crawler.arun(\n",
    "#             url=\"https://www.nbcnews.com/business\",\n",
    "#             bypass_cache=True,  # By default this is False, meaning the cache will be used\n",
    "#         )\n",
    "#         print(result.markdown[:500])  # Print the first 500 characters\n",
    "from crawl4ai import AsyncWebCrawler, CacheMode  # Add CacheMode import\n",
    "\n",
    "\n",
    "async def simple_crawl():\n",
    "    async with AsyncWebCrawler() as crawler:\n",
    "        result = await crawler.arun(\n",
    "            url=\"https://webscraper.io/test-sites\",\n",
    "            cache_mode=CacheMode.BYPASS,  # Updated from bypass_cache=True\n",
    "        )\n",
    "        print(\"result\", result.markdown[:500])\n",
    "\n",
    "\n",
    "asyncio.run(simple_crawl())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT].... → Crawl4AI 0.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7114/2200669308.py:14: DeprecationWarning: Cache control boolean flags are deprecated and will be removed in version X.X.X. Use 'cache_mode' parameter instead. Examples:\n",
      "- For bypass_cache=True, use cache_mode=CacheMode.BYPASS\n",
      "- For disable_cache=True, use cache_mode=CacheMode.DISABLED\n",
      "- For no_cache_read=True, use cache_mode=CacheMode.WRITE_ONLY\n",
      "- For no_cache_write=True, use cache_mode=CacheMode.READ_ONLY\n",
      "Pass warning=False to suppress this warning.\n",
      "  result = await crawler.arun(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FETCH]... ↓ https://www.nbcnews.com/business... | Status: True | Time: 9.90s\n",
      "[SCRAPE].. ◆ Processed https://www.nbcnews.com/business... | Time: 85ms\n",
      "[COMPLETE] ● https://www.nbcnews.com/business... | Status: True | Total: 9.99s\n",
      "IE 11 is not supported. For an optimal experience visit our site on another browser.\n",
      "\n",
      "Skip to Content\n",
      "\n",
      "[NBC News Logo](https://www.nbcnews.com)\n",
      "\n",
      "Sponsored By\n",
      "\n",
      "  * [Live: CEO Killing](https://www.nbcnews.com/news/us-news/live-blog/live-updates-man-questioned-unitedhealthcare-ceo-brian-thompsons-shoot-rcna183160)\n",
      "  * [U.S. News](https://www.nbcnews.com/us-news)\n",
      "  * Local\n",
      "  * [New York](https://www.nbcnews.com/new-york)\n",
      "  * [Los Angeles](https://www.nbcnews.com/los-angeles)\n",
      "  * [Chicago](https://ww\n"
     ]
    }
   ],
   "source": [
    "async def crawl_dynamic_content():\n",
    "    # You can use wait_for to wait for a condition to be met before returning the result\n",
    "    # wait_for = \"\"\"() => {\n",
    "    #     return Array.from(document.querySelectorAll('article.tease-card')).length > 10;\n",
    "    # }\"\"\"\n",
    "\n",
    "    # wait_for can be also just a css selector\n",
    "    # wait_for = \"article.tease-card:nth-child(10)\"\n",
    "\n",
    "    async with AsyncWebCrawler(verbose=True) as crawler:\n",
    "        js_code = [\n",
    "            \"const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More')); loadMoreButton && loadMoreButton.click();\"\n",
    "        ]\n",
    "        result = await crawler.arun(\n",
    "            url=\"https://www.nbcnews.com/business\",\n",
    "            js_code=js_code,\n",
    "            # wait_for=wait_for,\n",
    "            bypass_cache=True,\n",
    "        )\n",
    "        print(result.markdown[:500])  # Print first 500 characters\n",
    "\n",
    "\n",
    "asyncio.run(crawl_dynamic_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7114/4117242193.py:3: DeprecationWarning: Cache control boolean flags are deprecated and will be removed in version X.X.X. Use 'cache_mode' parameter instead. Examples:\n",
      "- For bypass_cache=True, use cache_mode=CacheMode.BYPASS\n",
      "- For disable_cache=True, use cache_mode=CacheMode.DISABLED\n",
      "- For no_cache_read=True, use cache_mode=CacheMode.WRITE_ONLY\n",
      "- For no_cache_write=True, use cache_mode=CacheMode.READ_ONLY\n",
      "Pass warning=False to suppress this warning.\n",
      "  result = await crawler.arun(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/strollr/crawlai/env/lib/python3.11/site-packages/crawl4ai/async_crawler_strategy.py:933\u001b[0m, in \u001b[0;36mAsyncPlaywrightCrawlerStrategy._crawl_web\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_only \u001b[38;5;129;01mand\u001b[39;00m (kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwait_for_images\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madjust_viewport_to_content\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)):\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;66;03m# Wait for network idle after initial load and images to load\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m page\u001b[38;5;241m.\u001b[39mwait_for_load_state(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnetworkidle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)                \n",
      "File \u001b[0;32m~/Documents/strollr/crawlai/env/lib/python3.11/site-packages/playwright/async_api/_generated.py:9069\u001b[0m, in \u001b[0;36mPage.wait_for_load_state\u001b[0;34m(self, state, timeout)\u001b[0m\n\u001b[1;32m   9025\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Page.wait_for_load_state\u001b[39;00m\n\u001b[1;32m   9026\u001b[0m \n\u001b[1;32m   9027\u001b[0m \u001b[38;5;124;03mReturns when the required load state has been reached.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9065\u001b[0m \u001b[38;5;124;03m    `page.set_default_timeout()` methods.\u001b[39;00m\n\u001b[1;32m   9066\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   9068\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mapping\u001b[38;5;241m.\u001b[39mfrom_maybe_impl(\n\u001b[0;32m-> 9069\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl_obj\u001b[38;5;241m.\u001b[39mwait_for_load_state(state\u001b[38;5;241m=\u001b[39mstate, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m   9070\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/strollr/crawlai/env/lib/python3.11/site-packages/playwright/_impl/_page.py:567\u001b[0m, in \u001b[0;36mPage.wait_for_load_state\u001b[0;34m(self, state, timeout)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait_for_load_state\u001b[39m(\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    564\u001b[0m     state: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdomcontentloaded\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnetworkidle\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    565\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    566\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_main_frame\u001b[38;5;241m.\u001b[39mwait_for_load_state(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlocals_to_params(\u001b[38;5;28mlocals\u001b[39m()))\n",
      "File \u001b[0;32m~/Documents/strollr/crawlai/env/lib/python3.11/site-packages/playwright/_impl/_frame.py:243\u001b[0m, in \u001b[0;36mFrame.wait_for_load_state\u001b[0;34m(self, state, timeout)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait_for_load_state\u001b[39m(\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    240\u001b[0m     state: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdomcontentloaded\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnetworkidle\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    241\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    242\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_load_state_impl(state, timeout)\n",
      "File \u001b[0;32m~/Documents/strollr/crawlai/env/lib/python3.11/site-packages/playwright/_impl/_frame.py:271\u001b[0m, in \u001b[0;36mFrame._wait_for_load_state_impl\u001b[0;34m(self, state, timeout)\u001b[0m\n\u001b[1;32m    266\u001b[0m     waiter\u001b[38;5;241m.\u001b[39mwait_for_event(\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_emitter,\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloadstate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    269\u001b[0m         handle_load_state_event,\n\u001b[1;32m    270\u001b[0m     )\n\u001b[0;32m--> 271\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m waiter\u001b[38;5;241m.\u001b[39mresult()\n",
      "File \u001b[0;32m/usr/lib/python3.11/asyncio/futures.py:287\u001b[0m, in \u001b[0;36mFuture.__await__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_asyncio_future_blocking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m/usr/lib/python3.11/asyncio/tasks.py:349\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 349\u001b[0m     \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/asyncio/futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "\u001b[0;31mTimeoutError\u001b[0m: Timeout 30000ms exceeded.\n=========================== logs ===========================\n\"load\" event fired\n============================================================",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFit Markdown Length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfit_markdown_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39mfit_markdown[:\u001b[38;5;241m1000\u001b[39m])\n\u001b[0;32m---> 17\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/strollr/crawlai/env/lib/python3.11/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/Documents/strollr/crawlai/env/lib/python3.11/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/asyncio/futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/usr/lib/python3.11/asyncio/tasks.py:277\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m, in \u001b[0;36mclean_content\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m AsyncWebCrawler() \u001b[38;5;28;01mas\u001b[39;00m crawler:\n\u001b[1;32m      3\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m crawler\u001b[38;5;241m.\u001b[39marun(\n\u001b[1;32m      4\u001b[0m         url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://janineintheworld.com/places-to-visit-in-central-mexico\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m         excluded_tags\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnav\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfooter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maside\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m         bypass_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m     )\n\u001b[0;32m---> 10\u001b[0m     full_markdown_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(result\u001b[38;5;241m.\u001b[39mmarkdown)\n\u001b[1;32m     11\u001b[0m     fit_markdown_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(result\u001b[38;5;241m.\u001b[39mfit_markdown)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull Markdown Length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_markdown_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "async def clean_content():\n",
    "    async with AsyncWebCrawler() as crawler:\n",
    "        result = await crawler.arun(\n",
    "            url=\"https://janineintheworld.com/places-to-visit-in-central-mexico\",\n",
    "            excluded_tags=[\"nav\", \"footer\", \"aside\"],\n",
    "            remove_overlay_elements=True,\n",
    "            word_count_threshold=10,\n",
    "            bypass_cache=True,\n",
    "        )\n",
    "        full_markdown_length = len(result.markdown)\n",
    "        fit_markdown_length = len(result.fit_markdown)\n",
    "        print(f\"Full Markdown Length: {full_markdown_length}\")\n",
    "        print(f\"Fit Markdown Length: {fit_markdown_length}\")\n",
    "        print(result.fit_markdown[:1000])\n",
    "\n",
    "\n",
    "asyncio.run(clean_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def link_analysis():\n",
    "    async with AsyncWebCrawler() as crawler:\n",
    "        result = await crawler.arun(\n",
    "            url=\"https://www.nbcnews.com/business\",\n",
    "            bypass_cache=True,\n",
    "            exclude_external_links=True,\n",
    "            exclude_social_media_links=True,\n",
    "            # exclude_domains=[\"facebook.com\", \"twitter.com\"]\n",
    "        )\n",
    "        print(f\"Found {len(result.links['internal'])} internal links\")\n",
    "        print(f\"Found {len(result.links['external'])} external links\")\n",
    "\n",
    "        for link in result.links[\"internal\"][:5]:\n",
    "            print(f\"Href: {link['href']}\\nText: {link['text']}\\n\")\n",
    "\n",
    "\n",
    "asyncio.run(link_analysis())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def media_handling():\n",
    "    async with AsyncWebCrawler() as crawler:\n",
    "        result = await crawler.arun(\n",
    "            url=\"https://www.nbcnews.com/business\",\n",
    "            bypass_cache=True,\n",
    "            exclude_external_images=False,\n",
    "            screenshot=True,\n",
    "        )\n",
    "        for img in result.media[\"images\"][:5]:\n",
    "            print(f\"Image URL: {img['src']}, Alt: {img['alt']}, Score: {img['score']}\")\n",
    "\n",
    "\n",
    "asyncio.run(media_handling())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def custom_hook_workflow():\n",
    "    async with AsyncWebCrawler() as crawler:\n",
    "        # Set a 'before_goto' hook to run custom code just before navigation\n",
    "        crawler.crawler_strategy.set_hook(\n",
    "            \"before_goto\", lambda page: print(\"[Hook] Preparing to navigate...\")\n",
    "        )\n",
    "\n",
    "        # Perform the crawl operation\n",
    "        result = await crawler.arun(url=\"https://crawl4ai.com\", bypass_cache=True)\n",
    "        print(result.markdown[:500])  # Display the first 500 characters\n",
    "\n",
    "\n",
    "asyncio.run(custom_hook_workflow())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
